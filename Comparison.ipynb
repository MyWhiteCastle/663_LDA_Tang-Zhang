{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Unigram Model\n",
    "\n",
    "To reproduce the model results on the simulated data, please follow the following instruction:\n",
    "1. Here we provide the code to preprocessing the simulated text file.\n",
    "2. For this model to run, dowload the text file generated by the program below, turn this txt file into Unix Executable file in your local terminal. Re-upload and run with UMM model.\n",
    "3. Go to Jupyter terminal, run \"$ python pDMM.py --corpus simu --ntopics 10 --twords 10 --niters 500 --name unigram\"\n",
    "4. Check the \"output\" folder and the file named \"unigram.topWords\" produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopword.txt\", 'r') as s:\n",
    "        stopwords = s.readlines()\n",
    "stpw = []\n",
    "for word in stopwords:\n",
    "    stpw.append(word.strip())\n",
    "with open(\"simulated.txt\") as f:\n",
    "    corpus = f.readlines()\n",
    "word_list = []\n",
    "for line in corpus:\n",
    "        if line != \"\":\n",
    "            before = line.strip().split()\n",
    "            #Remove stopwords from the strings\n",
    "            for word in before:\n",
    "                if word.lstrip(string.punctuation).rstrip(string.punctuation).lower() not in stpw:\n",
    "                    if word != \"\":\n",
    "                        word_list.append(word.lstrip(string.punctuation).rstrip(string.punctuation).strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for i in range(0,234,1):\n",
    "    line = \" \".join(word_list[i*5:i*5+5])\n",
    "    new.append(line)\n",
    "simu = \"\\n\".join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('simu.txt','w')\n",
    "output_file.write(simu)\n",
    "output_file.close()\n",
    "\n",
    "#Then turn this txt file into Unix Executable file in local terminal. Reupload and run with UMM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Mixture of Unigrams Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0: luffy piece search treasure head law monkey fruit named pirate\n",
    "\n",
    "Topic 1: devil fruit user fruits animals race sea power powers haki \n",
    "\n",
    "Topic 2: sea grand red water mountain half rain runs seas wind \n",
    "\n",
    "Topic 3: series pirates roger humans gol merry manga video animation produced \n",
    "\n",
    "Topic 4: crew luffy robin ancient liberates sabaody archipelago ace navy government \n",
    "\n",
    "Topic 5: crew blue pirates navy straw joins named grand east pirate \n",
    "\n",
    "Topic 6: luffy nami sanji arlong chopper body properties crew usopp encounters \n",
    "\n",
    "Topic 7: pose calm belts log developed called thirteen animated feature films \n",
    "\n",
    "Topic 8: piece manga pirates king set island zou series eiichiro history \n",
    "\n",
    "Topic 9: pirates straw island luffy hat grand kingdom magnetic island's fishman "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "This is an example of applying :class:`sklearn.decomposition.NMF` and\n",
    ":class:`sklearn.decomposition.LatentDirichletAllocation` on a corpus\n",
    "of documents and extract additive models of the topic structure of the\n",
    "corpus.  The output is a list of topics, each represented as a list of\n",
    "terms (weights are not shown).\n",
    "\n",
    "Non-negative Matrix Factorization is applied with two different objective\n",
    "functions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\n",
    "The latter is equivalent to Probabilistic Latent Semantic Indexing.\n",
    "\n",
    "The default parameters (n_samples / n_features / n_components) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is polynomial in NMF. In LDA, the time complexity is\n",
    "proportional to (n_samples * iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "n_samples = 16\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\txh06\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import LDApackage\n",
    "simulated_docs = LDApackage.read_documents_space('simulated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(simulated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NMF model\n",
    "nmf = sklearn.decomposition.NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "#print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "#print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics in NMF model (generalized Kullback-Leibler divergence):\n",
    "Topic 0: pirates line crew man search roger known king grand luffy\n",
    "\n",
    "Topic 1: history japan series date manga oda eiichiro body volumes world\n",
    "\n",
    "Topic 2: body devil animals used result users power presence time fruit\n",
    "\n",
    "Topic 3: grand time called sea currents line works island making specific\n",
    "\n",
    "Topic 4: law defeat mom sanji alliance caesar nami clown big straw\n",
    "\n",
    "Topic 5: group battles robin straw franky leading crew ancient save pluton\n",
    "\n",
    "Topic 6: used animals wind similar piece world various certain presence eiichiro\n",
    "\n",
    "Topic 7: ace huge grand luffy adoptive forced fish thousand led new\n",
    "\n",
    "Topic 8: soon cyborg island crew fishmen sabaody battle alias world archipelago\n",
    "\n",
    "Topic 9: blue humans usopp government going captures water sanji defeats creatures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
