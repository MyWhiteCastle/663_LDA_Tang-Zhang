{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to Simulated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling method\n",
    "First, we use the algorithm based on Gibbs sampling to accomplish our goal of summarizing 5 topics out of the \"One Piece\" document, and under each topic pick the top 10 words. This package is called \"LDApackage\". The final output is the file \"Gibbs_Sim_topwords.dat\". $\\textbf{If wish to reproduce results, see README for instructions.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/jovyan/work/sta-663-2019/projects/LDA\n",
      "Building wheels for collected packages: LDApackage\n",
      "  Building wheel for LDApackage (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-a1rd_c8w/wheels/e6/03/d4/b04aaa3df510f60cbe959e49b4d2be1f0b88341f429548e7c5\n",
      "Successfully built LDApackage\n",
      "Installing collected packages: LDApackage\n",
      "  Found existing installation: LDApackage 1.0\n",
      "    Uninstalling LDApackage-1.0:\n",
      "      Successfully uninstalled LDApackage-1.0\n",
      "Successfully installed LDApackage-1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import LDApackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile = 'simulated.txt'\n",
    "dpre = LDApackage.preprocessing(datafile)\n",
    "lda = LDApackage.LDAModel(dpre)\n",
    "lda.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To view the final output, please see \"topwords.dat\" in the directory. (Results would be rewritten each time running the code with different/same data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### VIEM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get documents \n",
    "simulated_docs = LDApackage.read_documents_space('simulated.txt')\n",
    "\n",
    "#get topic and words\n",
    "alpha, log_beta, topicwords = LDApackage.LDA_VIEM(simulated_docs,10,10)\n",
    "\n",
    "#save topic words to txt file\n",
    "with open('VIEM_Sim_topwords.txt', 'w') as f:\n",
    "    for item in topicwords:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion & Conclusion\n",
    "\n",
    "The simulated data is a very short piece of text, thus is easier to eye-spect the generated topics from the algorithms, but more difficult to summarize more valuable and specific topic distributions.\n",
    "\n",
    "From the results generated by both Gibbs Sampling and VIEM, it is not hard to tell that the words under topics are similar to each other. Taking a closer look, the words distributed under each topic actually make quite a lot of sense.\n",
    "\n",
    "For example, topic 6 from the Gibbs Sampling result (manga, series and etc.) are all words related to the nature and attributes of the manga/anime industry. Similar topic modeling is shown in the output from EM algorithm. Another example is that, the topic including \"Luffy,\" \"pirate,\" \"island\" and etc., are the words related to the main content and background of the One Piece manga/anime itself.\n",
    "\n",
    "It is also quite obvious that the model and either algorithm is able to process different forms/roots of words. Fortunately, different forms of such words are usually put into the same topic, showing the impressive accuracy of the model. For example, both algorithms put \"island\" and its plural form \"islands\" in the same topic, even though they fail to recognize that they are actually the same word. Because of the complexity of natural language, this shortcoming is understandable and expected.\n",
    "\n",
    "Since the dataset is quite small, there's no significant difference in terms of speed and accuracy between the initial and optimized algorithms. Application and discussions on real data can be found under \"Real+Data.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
